{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b300b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
    "\n",
    "# i have the environment, i can take actions in that environment\n",
    "# now i need a policy i.e. a way of behaving in this environment\n",
    "# given a state, i want to know the next action to take in the environment\n",
    "\n",
    "# env.reset()\n",
    "# while True:\n",
    "#     # replace the random action sample with a policy\n",
    "#     action = env.action_space.sample()\n",
    "#     env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a135cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a6016b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 96, 3), 7.042857142857144, False, False, {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4fcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2  # You might need: pip install opencv-python\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "# --- Custom Wrappers for Stability and Speed ---\n",
    "\n",
    "class RepeatActionAndMaxFrame(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Action Skipping: The agent acts every 'skip' frames.\n",
    "    We return the max intensity of the last two frames to deal with \n",
    "    rendering flickering (common in Atari/Gym games).\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(RepeatActionAndMaxFrame, self).__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "                break\n",
    "        \n",
    "        return obs, total_reward, terminated, truncated, info\n",
    "\n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    1. Cut out the bottom status bar (irrelevant text/numbers).\n",
    "    2. Convert to Grayscale (color doesn't matter for the track, saves memory).\n",
    "    3. Resize to 84x84 (standard DQN input size).\n",
    "    4. Normalize 0-1.\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(84, 84), dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Crop: Remove bottom status bar (96x96 -> keeping top 84 rows)\n",
    "        # CarRacing-v3 is 96x96. We crop to remove the bar at the bottom.\n",
    "        # Actually, let's just resize the whole thing to 84x84 directly \n",
    "        # but grayscale it first.\n",
    "        \n",
    "        # RGB -> Gray using standard weights\n",
    "        # obs shape: (H, W, 3)\n",
    "        gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Resize to 84x84\n",
    "        resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Normalize to 0-1\n",
    "        norm = resized / 255.0\n",
    "        \n",
    "        return norm\n",
    "\n",
    "class StackFrames(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Stack k last frames. Returns a (k, 84, 84) array.\n",
    "    This gives the agent a sense of VELOCITY.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, stack_size=4):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        self.stack_size = stack_size\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "        \n",
    "        # Update observation space\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1, \n",
    "            shape=(stack_size, old_space.shape[0], old_space.shape[1]), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array(self.frames)\n",
    "\n",
    "def make_env():\n",
    "    # Helper to combine all wrappers\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
    "    env = RepeatActionAndMaxFrame(env, skip=4)\n",
    "    env = PreprocessFrame(env)\n",
    "    env = StackFrames(env, stack_size=4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5cd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_size, input_channels=4):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Input shape: (Batch, 4, 84, 84)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # 84x84 image -> Conv1 -> 20x20 -> Conv2 -> 9x9 -> Conv3 -> 7x7\n",
    "        # 64 filters * 7 * 7 = 3136\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97288c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Training Started (Press Ctrl+C to stop and save)...\n",
      "Episode 0 | Score: -35.71 | Epsilon: 0.998\n",
      "Episode 1 | Score: -15.79 | Epsilon: 0.996\n",
      "Episode 2 | Score: -34.66 | Epsilon: 0.993\n",
      "Episode 3 | Score: -25.20 | Epsilon: 0.991\n",
      "Episode 4 | Score: -22.39 | Epsilon: 0.989\n",
      "Episode 5 | Score: -21.15 | Epsilon: 0.987\n",
      "Episode 6 | Score: -21.15 | Epsilon: 0.984\n",
      "Episode 7 | Score: -70.78 | Epsilon: 0.982\n",
      "Episode 8 | Score: -13.49 | Epsilon: 0.980\n",
      "Episode 9 | Score: -47.19 | Epsilon: 0.978\n",
      "Episode 10 | Score: -33.59 | Epsilon: 0.976\n",
      "Episode 11 | Score: -37.98 | Epsilon: 0.973\n",
      "Episode 12 | Score: -75.31 | Epsilon: 0.971\n",
      "Episode 13 | Score: -28.57 | Epsilon: 0.969\n",
      "Episode 14 | Score: -62.96 | Epsilon: 0.967\n",
      "Episode 15 | Score: -60.63 | Epsilon: 0.965\n",
      "Episode 16 | Score: -42.86 | Epsilon: 0.963\n",
      "Episode 17 | Score: -48.72 | Epsilon: 0.960\n",
      "Episode 18 | Score: -53.07 | Epsilon: 0.958\n",
      "Episode 19 | Score: -35.15 | Epsilon: 0.956\n",
      "Episode 20 | Score: -57.75 | Epsilon: 0.954\n",
      "Episode 21 | Score: -63.09 | Epsilon: 0.952\n",
      "Episode 22 | Score: -31.60 | Epsilon: 0.950\n",
      "Episode 23 | Score: -63.30 | Epsilon: 0.948\n",
      "Episode 24 | Score: -4.76 | Epsilon: 0.945\n",
      "Episode 25 | Score: -21.63 | Epsilon: 0.943\n",
      "Episode 26 | Score: -23.34 | Epsilon: 0.941\n",
      "Episode 27 | Score: -77.64 | Epsilon: 0.939\n",
      "Episode 28 | Score: -34.48 | Epsilon: 0.937\n",
      "Episode 29 | Score: -24.66 | Epsilon: 0.935\n",
      "Episode 30 | Score: -68.64 | Epsilon: 0.933\n",
      "Episode 31 | Score: -55.41 | Epsilon: 0.931\n",
      "Episode 32 | Score: -67.39 | Epsilon: 0.929\n",
      "Episode 33 | Score: -69.59 | Epsilon: 0.927\n",
      "Episode 34 | Score: -57.60 | Epsilon: 0.925\n",
      "Episode 35 | Score: -65.64 | Epsilon: 0.923\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "BATCH_SIZE = 128         # Higher batch size for stable gradients\n",
    "GAMMA = 0.99             # Discount future rewards\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1            # Don't go to 0.05 too fast, keep exploring a bit\n",
    "EPS_DECAY = 100000       # Slower decay! This game takes time to learn.\n",
    "LR = 1e-4\n",
    "TARGET_UPDATE = 1000     # Hard update every 1000 steps\n",
    "MEMORY_SIZE = 50000      \n",
    "NUM_EPISODES = 1000      # 600-1000 episodes usually gets decent results\n",
    "\n",
    "# Replay Buffer (Same as before but handles numpy stacks)\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- Main Setup ---\n",
    "env = make_env()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = QNetwork(n_actions).to(device)\n",
    "target_net = QNetwork(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayBuffer(MEMORY_SIZE)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "print(\"Training Started (Press Ctrl+C to stop and save)...\")\n",
    "\n",
    "try:\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        # CarRacing has a negative reward per frame (-0.1).\n",
    "        # We need to encourage it to find the gas pedal early on.\n",
    "        \n",
    "        while True:\n",
    "            # 1. Epsilon Greedy Action\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            \n",
    "            if random.random() < eps_threshold:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    action = policy_net(state_t).max(1)[1].item()\n",
    "            \n",
    "            steps_done += 1\n",
    "\n",
    "            # 2. Step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # 3. Store\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # 4. Optimize\n",
    "            if len(memory) > BATCH_SIZE:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(BATCH_SIZE)\n",
    "\n",
    "                state_batch = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "                action_batch = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "                reward_batch = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "                next_state_batch = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "                done_batch = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "                q_values = policy_net(state_batch).gather(1, action_batch)\n",
    "                \n",
    "                # Double DQN logic often helps, but standard DQN is fine here.\n",
    "                # Standard Target Calculation:\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
    "                    expected_q_values = reward_batch + (GAMMA * next_q_values * (1 - done_batch))\n",
    "\n",
    "                loss = F.smooth_l1_loss(q_values, expected_q_values)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Gradient clipping prevents \"exploding gradients\" which ruin training\n",
    "                for param in policy_net.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "                optimizer.step()\n",
    "\n",
    "            if steps_done % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"Episode {i_episode} | Score: {score:.2f} | Epsilon: {eps_threshold:.3f}\")\n",
    "        \n",
    "        # Save periodically\n",
    "        if i_episode % 50 == 0:\n",
    "            torch.save(policy_net.state_dict(), \"car_racing_dqn.pth\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving current model...\")\n",
    "    torch.save(policy_net.state_dict(), \"car_racing_dqn_interrupted.pth\")\n",
    "\n",
    "print(\"Training Finished.\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INFERENCE ---\n",
    "def watch_agent():\n",
    "    # 1. Setup exact same environment wrappers\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
    "    env = RepeatActionAndMaxFrame(env, skip=4)\n",
    "    env = PreprocessFrame(env)\n",
    "    env = StackFrames(env, stack_size=4)\n",
    "    \n",
    "    # 2. Load Model\n",
    "    model = QNetwork(env.action_space.n).to(device)\n",
    "    model.load_state_dict(torch.load(\"car_racing_dqn.pth\", map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # 3. Play\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"Watching Agent...\")\n",
    "    while True:\n",
    "        state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = model(state_t).max(1)[1].item()\n",
    "            \n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Render is handled by gym window usually\n",
    "        # If running in Colab/Headless, you need specific video recording wrappers\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            print(f\"Game Over. Total Reward: {total_reward}\")\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "# watch_agent() # Uncomment to run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
